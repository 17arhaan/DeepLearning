{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
       "# Cat-Dog Classification with L2 Regularization and Accuracy Plotting\n",
       "\n",
       "This notebook demonstrates how to implement L2 regularization in two ways on a simple convolutional neural network (CNN) for cat-dog classification:\n",
       "\n",
       "1. **Using the optimizer's weight decay parameter**\n",
       "2. **Manually adding the L2 norm penalty in the training loop**\n",
       "\n",
       "It also tracks the training accuracy and the L2 norm of weights over epochs, and plots a graph at the end to help you observe the impact of the regularization on the training performance.\n",
       "\n",
       "## Using This Notebook with a Custom Dataset\n",
       "\n",
       "If you have a custom dataset, make sure it is organized in a structure that is compatible with PyTorch's `ImageFolder` class. Typically, this means you should have a root directory with one subdirectory per class (e.g., `cat` and `dog`). For example:\n",
       "\n",
       "```\n",
       "your_dataset_root/\n",
       "    ├── cat/\n",
       "    │     ├── image1.jpg\n",
       "    │     ├── image2.jpg\n",
       "    │     └── ...\n",
       "    └── dog/\n",
       "          ├── image1.jpg\n",
       "          ├── image2.jpg\n",
       "          └── ...\n",
       "```\n",
       "\n",
       "Then, change the `root` parameter in the `datasets.ImageFolder` call (see the code cell below) to point to your custom dataset directory.\n",
       "\n",
       "No data augmentation is applied here; only basic resizing and normalization are used."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup-imports",
      "metadata": {},
      "outputs": [],
      "source": [
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "from torchvision import datasets, transforms\n",
       "from torch.utils.data import DataLoader\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Check device\n",
       "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
       "print(f\"Using device: {device}\")\n",
       "\n",
       "# Define a simple CNN for binary classification (cats vs dogs)\n",
       "class CatDogCNN(nn.Module):\n",
       "    def __init__(self):\n",
       "        super(CatDogCNN, self).__init__()\n",
       "        self.features = nn.Sequential(\n",
       "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
       "            nn.ReLU(),\n",
       "            nn.MaxPool2d(2),\n",
       "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
       "            nn.ReLU(),\n",
       "            nn.MaxPool2d(2)\n",
       "        )\n",
       "        self.classifier = nn.Sequential(\n",
       "            nn.Flatten(),\n",
       "            nn.Linear(32 * 56 * 56, 128),  # assuming input images are 224x224\n",
       "            nn.ReLU(),\n",
       "            nn.Linear(128, 2)  # 2 output classes: cat and dog\n",
       "        )\n",
       "\n",
       "    def forward(self, x):\n",
       "        x = self.features(x)\n",
       "        x = self.classifier(x)\n",
       "        return x\n",
       "\n",
       "# Transformation: only basic resizing and normalization, no data augmentation.\n",
       "transform = transforms.Compose([\n",
       "    transforms.Resize((224, 224)),\n",
       "    transforms.ToTensor(),\n",
       "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
       "])\n",
       "\n",
       "# Change 'data/train' to the path of your custom dataset if needed.\n",
       "train_dataset = datasets.ImageFolder(root='data/train', transform=transform)\n",
       "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
       "\n",
       "print(f\"Number of training samples: {len(train_dataset)}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "exp_a",
      "metadata": {},
      "source": [
       "## Experiment (a): L2 Regularization via Optimizer’s Weight Decay\n",
       "\n",
       "This experiment uses the optimizer's built-in weight decay to apply L2 regularization. Adjust the `weight_decay` parameter to see its effect on the training dynamics. The training function now also tracks training accuracy and the L2 norm of weights."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "exp_a_code",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create model instance for Experiment (a)\n",
       "model_a = CatDogCNN().to(device)\n",
       "\n",
       "# Loss function\n",
       "criterion = nn.CrossEntropyLoss()\n",
       "\n",
       "# Set weight decay factor (lambda)\n",
       "weight_decay = 1e-4\n",
       "\n",
       "# Use an optimizer with built-in weight decay\n",
       "optimizer_a = optim.Adam(model_a.parameters(), lr=1e-3, weight_decay=weight_decay)\n",
       "\n",
       "def train_model_a(num_epochs=5):\n",
       "    model_a.train()\n",
       "    epoch_accuracy = []\n",
       "    weight_norms = []\n",
       "    for epoch in range(num_epochs):\n",
       "        running_loss = 0.0\n",
       "        correct = 0\n",
       "        total = 0\n",
       "        for images, labels in train_loader:\n",
       "            images, labels = images.to(device), labels.to(device)\n",
       "            \n",
       "            optimizer_a.zero_grad()\n",
       "            outputs = model_a(images)\n",
       "            loss = criterion(outputs, labels)\n",
       "            loss.backward()\n",
       "            optimizer_a.step()\n",
       "            \n",
       "            running_loss += loss.item() * images.size(0)\n",
       "            \n",
       "            # Compute accuracy\n",
       "            _, predicted = torch.max(outputs, 1)\n",
       "            total += labels.size(0)\n",
       "            correct += (predicted == labels).sum().item()\n",
       "        \n",
       "        epoch_loss = running_loss / len(train_dataset)\n",
       "        acc = 100 * correct / total\n",
       "        epoch_accuracy.append(acc)\n",
       "        \n",
       "        # Get L2 norm of the first conv layer weights\n",
       "        weight_norm = model_a.features[0].weight.data.norm(2).item()\n",
       "        weight_norms.append(weight_norm)\n",
       "        \n",
       "        print(f\"[Exp A] Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {acc:.2f}%\")\n",
       "        print(f\"[Exp A] Layer Conv1 L2 norm: {weight_norm:.4f}\")\n",
       "    \n",
       "    return epoch_accuracy, weight_norms\n",
       "\n",
       "# Uncomment the line below to run Experiment (a) and capture the accuracy and weight norms\n",
       "# acc_a, l2_a = train_model_a()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "exp_b",
      "metadata": {},
      "source": [
       "## Experiment (b): L2 Regularization by Manually Adding the L2 Norm\n",
       "\n",
       "In this experiment, we apply L2 regularization manually by computing the L2 norm of all weight parameters and adding it to the loss. The training function tracks training accuracy and the L2 norm of weights as well."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "exp_b_code",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create a new instance of the model for Experiment (b)\n",
       "model_b = CatDogCNN().to(device)\n",
       "\n",
       "# Use the same loss function\n",
       "criterion = nn.CrossEntropyLoss()\n",
       "\n",
       "# Create an optimizer without built-in weight decay\n",
       "optimizer_b = optim.Adam(model_b.parameters(), lr=1e-3, weight_decay=0)  # weight_decay is 0 for manual regularization\n",
       "\n",
       "# Regularization strength\n",
       "lambda_reg = 1e-4\n",
       "\n",
       "def train_model_b(num_epochs=5):\n",
       "    model_b.train()\n",
       "    epoch_accuracy = []\n",
       "    weight_norms = []\n",
       "    for epoch in range(num_epochs):\n",
       "        running_loss = 0.0\n",
       "        correct = 0\n",
       "        total = 0\n",
       "        for images, labels in train_loader:\n",
       "            images, labels = images.to(device), labels.to(device)\n",
       "            \n",
       "            optimizer_b.zero_grad()\n",
       "            outputs = model_b(images)\n",
       "            loss = criterion(outputs, labels)\n",
       "            \n",
       "            # Manually compute the L2 regularization penalty\n",
       "            l2_penalty = 0.0\n",
       "            for param in model_b.parameters():\n",
       "                if param.requires_grad:\n",
       "                    l2_penalty += torch.sum(param ** 2)\n",
       "            \n",
       "            loss += lambda_reg * l2_penalty\n",
       "            loss.backward()\n",
       "            optimizer_b.step()\n",
       "            \n",
       "            running_loss += loss.item() * images.size(0)\n",
       "            \n",
       "            # Compute accuracy\n",
       "            _, predicted = torch.max(outputs, 1)\n",
       "            total += labels.size(0)\n",
       "            correct += (predicted == labels).sum().item()\n",
       "        \n",
       "        epoch_loss = running_loss / len(train_dataset)\n",
       "        acc = 100 * correct / total\n",
       "        epoch_accuracy.append(acc)\n",
       "        \n",
       "        # Get L2 norm of the first conv layer weights\n",
       "        weight_norm = model_b.features[0].weight.data.norm(2).item()\n",
       "        weight_norms.append(weight_norm)\n",
       "        \n",
       "        print(f\"[Exp B] Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {acc:.2f}%\")\n",
       "        print(f\"[Exp B] Layer Conv1 L2 norm: {weight_norm:.4f}\")\n",
       "    \n",
       "    return epoch_accuracy, weight_norms\n",
       "\n",
       "# Uncomment the line below to run Experiment (b) and capture the accuracy and weight norms\n",
       "# acc_b, l2_b = train_model_b()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "plotting",
      "metadata": {},
      "source": [
       "## Plotting Accuracy and Weight Norms\n",
       "\n",
       "After running one of the experiments (either Experiment (a) or (b)), run the cell below to plot the training accuracy (and optionally, the L2 norm of the weights) versus epoch. Adjust which experiment's data you want to plot by uncommenting the appropriate lines."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot_code",
      "metadata": {},
      "outputs": [],
      "source": [
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Example: Plot results for Experiment (a)\n",
       "# Make sure to run train_model_a() and obtain acc_a, l2_a first by uncommenting the call in the cell above\n",
       "\n",
       "# Uncomment one of these lines depending on which experiment you ran:\n",
       "# For Experiment (a):\n",
       "# acc_history, l2_history = acc_a, l2_a\n",
       "\n",
       "# For Experiment (b):\n",
       "# acc_history, l2_history = acc_b, l2_b\n",
       "\n",
       "# For demonstration, if you haven't run either experiment, here is some dummy data:\n",
       "if 'acc_a' not in globals() and 'acc_b' not in globals():\n",
       "    acc_history = [60, 65, 70, 75, 80]\n",
       "    l2_history = [100, 95, 90, 85, 80]\n",
       "else:\n",
       "    try:\n",
       "        acc_history, l2_history = acc_a, l2_a\n",
       "    except NameError:\n",
       "        acc_history, l2_history = acc_b, l2_b\n",
       "\n",
       "# Plot training accuracy\n",
       "epochs = range(1, len(acc_history)+1)\n",
       "plt.figure()\n",
       "plt.plot(epochs, acc_history, marker='o', label='Training Accuracy (%)')\n",
       "plt.title('Training Accuracy per Epoch')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Accuracy (%)')\n",
       "plt.legend()\n",
       "plt.grid(True)\n",
       "plt.show()\n",
       "\n",
       "# Optionally, plot the L2 norm of weights\n",
       "plt.figure()\n",
       "plt.plot(epochs, l2_history, marker='o', color='red', label='Layer Conv1 L2 Norm')\n",
       "plt.title('L2 Norm of Conv1 Weights per Epoch')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('L2 Norm')\n",
       "plt.legend()\n",
       "plt.grid(True)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
       "## Conclusion\n",
       "\n",
       "This notebook provided two experiments demonstrating L2 regularization in a CNN for cat-dog classification. Experiment (a) uses the optimizer's built-in weight decay, while Experiment (b) computes the L2 norm manually and adds it to the loss. Both experiments track training accuracy and the L2 norm of the weights during training. In the final cell, you can plot these metrics to observe how regularization affects model training.\n",
       "\n",
       "Feel free to adjust hyperparameters (learning rate, regularization strength, etc.) and the model architecture to best suit your custom dataset and application."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": "3.x"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   