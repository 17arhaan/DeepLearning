{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "87c4dbea-4b10-40a0-bd6f-b344db0d4a64",
      "metadata": {},
      "source": [
       "# Next Character Predictor using LSTM in PyTorch\n",
       "\n",
       "This notebook demonstrates how to build an LSTM-based next-character predictor. It uses a sample text (you can replace it with your own corpus) to build a character vocabulary, prepares training sequences, defines the LSTM model, trains the model to predict the next character given a sequence of previous characters, and finally generates text based on a seed string.\n",
       "\n",
       "Adjust hyperparameters (e.g., sequence length, embedding size, hidden size, learning rate, number of epochs) as needed."
      ]
     },
     {
      "cell_type": "code",
      "id": "4a96b0e6-6d48-4a2d-9d0e-ded1b2e91d03",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "import numpy as np\n",
       "import random\n",
       "import sys\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Set device\n",
       "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
       "print(\"Using device:\", device)"
      ]
     },
     {
      "cell_type": "code",
      "id": "cfe15084-8339-4885-a8c3-67c14f33f4a2",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
       "# -------------------------------\n",
       "# 1. Data Preparation\n",
       "# -------------------------------\n",
       "\n",
       "# Sample text for demonstration; replace with your own text if desired\n",
       "text = (\n",
       "    \"In the beginning God created the heaven and the earth. \"\n",
       "    \"And the earth was without form, and void; and darkness was upon the face of the deep. \"\n",
       "    \"And the Spirit of God moved upon the face of the waters. \"\n",
       "    \"And God said, Let there be light: and there was light.\"\n",
       ")\n",
       "\n",
       "# Build the character vocabulary\n",
       "chars = sorted(list(set(text)))\n",
       "vocab_size = len(chars)\n",
       "print(\"Unique characters:\", vocab_size)\n",
       "print(\"Characters:\", chars)\n",
       "\n",
       "# Create mappings from characters to indices and vice versa\n",
       "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
       "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
       "\n",
       "# Convert the text into a sequence of indices\n",
       "text_indices = np.array([char2idx[ch] for ch in text])\n",
       "\n",
       "# Define sequence length: using last 10 characters as input to predict the next character\n",
       "sequence_length = 10\n",
       "\n",
       "def create_sequences(data, seq_length):\n",
       "    inputs = []\n",
       "    targets = []\n",
       "    for i in range(len(data) - seq_length):\n",
       "        inputs.append(data[i:i+seq_length])\n",
       "        targets.append(data[i+seq_length])\n",
       "    return np.array(inputs), np.array(targets)\n",
       "\n",
       "inputs, targets = create_sequences(text_indices, sequence_length)\n",
       "print(\"Total sequences:\", len(inputs))"
      ]
     },
     {
      "cell_type": "code",
      "id": "d5866b50-cd2a-40da-a3cb-fb9ab5fbd1cb",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
       "# -------------------------------\n",
       "# 2. Dataset and DataLoader\n",
       "# -------------------------------\n",
       "\n",
       "from torch.utils.data import Dataset, DataLoader\n",
       "\n",
       "class TextDataset(Dataset):\n",
       "    def __init__(self, inputs, targets):\n",
       "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
       "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
       "    \n",
       "    def __len__(self):\n",
       "        return len(self.inputs)\n",
       "    \n",
       "    def __getitem__(self, idx):\n",
       "        return self.inputs[idx], self.targets[idx]\n",
       "\n",
       "dataset = TextDataset(inputs, targets)\n",
       "batch_size = 64\n",
       "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
     },
     {
      "cell_type": "code",
      "id": "c2a8267a-20cf-4f3e-b5d4-5c994c57b5a8",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
       "# -------------------------------\n",
       "# 3. Define the LSTM Model\n",
       "# -------------------------------\n",
       "\n",
       "class LSTMNextChar(nn.Module):\n",
       "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
       "        super(LSTMNextChar, self).__init__()\n",
       "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
       "        self.lstm = nn.LSTM(input_size=embed_size,\n",
       "                            hidden_size=hidden_size,\n",
       "                            num_layers=num_layers,\n",
       "                            batch_first=True)\n",
       "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
       "    \n",
       "    def forward(self, x, hidden=None):\n",
       "        # x shape: (batch_size, sequence_length)\n",
       "        x = self.embed(x)  # (batch_size, sequence_length, embed_size)\n",
       "        output, hidden = self.lstm(x, hidden)  # output: (batch_size, seq_length, hidden_size)\n",
       "        output = output[:, -1, :]  # take output from the last time step\n",
       "        logits = self.fc(output)  # (batch_size, vocab_size)\n",
       "        return logits, hidden\n",
       "\n",
       "embed_size = 32\n",
       "hidden_size = 128\n",
       "num_layers = 1\n",
       "\n",
       "model = LSTMNextChar(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
       "print(model)"
      ]
     },
     {
      "cell_type": "code",
      "id": "53a058ac-3b05-44f4-9af9-80d5a4143e72",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
       "# -------------------------------\n",
       "# 4. Training Setup and Loop\n",
       "# -------------------------------\n",
       "\n",
       "criterion = nn.CrossEntropyLoss()\n",
       "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
       "num_epochs = 200\n",
       "\n",
       "model.train()\n",
       "for epoch in range(num_epochs):\n",
       "    total_loss = 0\n",
       "    for batch_inputs, batch_targets in data_loader:\n",
       "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
       "        optimizer.zero_grad()\n",
       "        logits, _ = model(batch_inputs)\n",
       "        loss = criterion(logits, batch_targets)\n",
       "        loss.backward()\n",
       "        optimizer.step()\n",
       "        total_loss += loss.item()\n",
       "    \n",
       "    if (epoch + 1) % 20 == 0:\n",
       "        avg_loss = total_loss / len(data_loader)\n",
       "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
       "\n",
       "print(\"Training complete.\")"
      ]
     },
     {
      "cell_type": "code",
      "id": "8bf7a2ad-138e-47c5-8c93-ec85a7af99ad",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
       "# -------------------------------\n",
       "# 5. Text Generation Function\n",
       "# -------------------------------\n",
       "\n",
       "def generate_text(model, seed_text, length=100, temperature=1.0):\n",
       "    \"\"\"\n",
       "    Generate text given a seed string.\n",
       "    \n",
       "    Args:\n",
       "        model: Trained LSTM model\n",
       "        seed_text: Initial text string (length >= sequence_length)\n",
       "        length: Number of characters to generate\n",
       "        temperature: Controls randomness (higher -> more random)\n",
       "    \n",
       "    Returns:\n",
       "        Generated text string\n",
       "    \"\"\"\n",
       "    model.eval()\n",
       "    generated = seed_text\n",
       "    # Convert seed_text to indices (take only the last `sequence_length` characters)\n",
       "    input_seq = [char2idx[ch] for ch in seed_text[-sequence_length:]]\n",
       "    input_seq = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
       "    hidden = None\n",
       "    \n",
       "    for _ in range(length):\n",
       "        logits, hidden = model(input_seq, hidden)\n",
       "        logits = logits / temperature\n",
       "        probs = torch.softmax(logits, dim=-1).detach().cpu().numpy().squeeze()\n",
       "        next_idx = np.random.choice(len(probs), p=probs)\n",
       "        next_char = idx2char[next_idx]\n",
       "        generated += next_char\n",
       "        \n",
       "        # Update input sequence: drop first element and append the predicted index\n",
       "        new_input = torch.tensor([[next_idx]], dtype=torch.long).to(device)\n",
       "        input_seq = torch.cat([input_seq[:, 1:], new_input], dim=1)\n",
       "    \n",
       "    return generated\n",
       "\n",
       "# Generate sample text\n",
       "seed = \"And God s\"  # Ensure the seed length is at least `sequence_length`\n",
       "generated_text = generate_text(model, seed, length=200, temperature=0.8)\n",
       "print(\"Generated Text:\\n\")\n",
       "print(generated_text)"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": "3.x"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   